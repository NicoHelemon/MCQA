{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0fca694f-9de6-4eaf-8983-06a89ee5c451",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "openbookqa: selected 4900 of 4957\n",
      "sciq: selected 10000 of 11679\n",
      "mmlu_aux: selected 85100 of 99842\n",
      "aqua_rat: selected 50000 of 97467\n",
      "medmcqa: selected 50000 of 182822\n",
      "openbookqa val: selected 12 of 500\n",
      "sciq val: selected 26 of 1000\n",
      "mmlu_aux val: selected 221 of 1531\n",
      "aqua_rat val: selected 130 of 254\n",
      "medmcqa val: selected 130 of 4183\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e700b4edf44f43609743634eae00d75f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4900 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "132752193b11451d8e7c207bde48555f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d9668c6837648f1876e5983ff5b56ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/85100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39be1e267e7243ab926c9d8690bfd401",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70b410ea9dab4581a3655b1376eff139",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unified examples: 200000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5d1f6b32fd54ce9bdebcfed8b707b0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/12 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cac79fe3f8e4e73ae3be8227b595066",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/26 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3cacbb7e133449e8cd2405e3d1ab685",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/221 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd5e82d9b7fc482eab636df0f19c3c65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/130 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "556b59f237904568bfcc09b6ada06a3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/130 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unified val examples: 519\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb18498c69424b84ba3fffcf14947d6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8fb1fc34ba04bc4be03be051717172b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/200 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9fa266f47294098bb410191476ef451",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf91aead028b435e9d89f4060e1bcc0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98a87cc6bee64408afc3fa5d8c8b1880",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/585 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pushed train + validation splits to NicoHelemon/MNLP_M2_mcqa_dataset\n",
      "Dataset successfully pushed to NicoHelemon/MNLP_M2_mcqa_dataset\n"
     ]
    }
   ],
   "source": [
    "# Script to load, sample, unify, and push MCQA datasets to HuggingFace Hub\n",
    "# Replace with your huggingface username\n",
    "\n",
    "HF_USERNAME = \"NicoHelemon\"\n",
    "REPO_ID = f\"{HF_USERNAME}/MNLP_M2_mcqa_dataset\"\n",
    "\n",
    "import os\n",
    "os.environ[\"HF_TOKEN\"] = \"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\"\n",
    "\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "import random\n",
    "from collections import deque\n",
    "\n",
    "# Desired subset sizes per dataset\n",
    "SUBSETS = {\n",
    "    'openbookqa': 4900,\n",
    "    'sciq':      10000,\n",
    "    #'race':      50000,\n",
    "    'mmlu_aux':  85100,\n",
    "    'aqua_rat':  50000,\n",
    "    'medmcqa' :  50000\n",
    "}\n",
    "\n",
    "# SUBSETS = {\n",
    "#     'openbookqa': 10,\n",
    "#     'sciq':       10,\n",
    "#     #'race':       10,\n",
    "#     'mmlu_aux':   10,\n",
    "#     'aqua_rat':   10,\n",
    "#     'medmcqa' :   10\n",
    "# }\n",
    "\n",
    "# 1. Load each dataset split (train) and sample subset\n",
    "raw_datasets = {}\n",
    "raw_datasets['openbookqa'] = load_dataset(\"allenai/openbookqa\", name=\"additional\", split='train')\n",
    "raw_datasets['sciq']       = load_dataset(\"allenai/sciq\", split='train')\n",
    "#raw_datasets['race']       = load_dataset(\"ehovy/race\", 'all', split='train')\n",
    "raw_datasets['mmlu_aux']   = load_dataset(\"cais/mmlu\", name=\"all\", split='auxiliary_train')\n",
    "raw_datasets['aqua_rat']   = load_dataset(\"deepmind/aqua_rat\", split='train')\n",
    "raw_datasets['medmcqa']    = load_dataset(\"openlifescienceai/medmcqa\", split='train')\n",
    "\n",
    "raw_val_datasets = {\n",
    "    'openbookqa': load_dataset(\"allenai/openbookqa\", name=\"additional\", split='validation'),\n",
    "    'sciq':       load_dataset(\"allenai/sciq\", split='validation'),\n",
    "    #'race':       load_dataset(\"ehovy/race\", 'all', split='validation'),\n",
    "    'mmlu_aux':   load_dataset(\"cais/mmlu\", name=\"all\", split='validation'),\n",
    "    'aqua_rat':   load_dataset(\"deepmind/aqua_rat\", split='validation'),\n",
    "    'medmcqa' :   load_dataset(\"openlifescienceai/medmcqa\", split='validation')\n",
    "}\n",
    "\n",
    "actual_ratios = {name: len(raw_val_datasets[name]) / len(raw_datasets[name]) for name in raw_datasets}\n",
    "val_ratio = min(0.05, min(actual_ratios.values()))\n",
    "\n",
    "VAL_SUBSETS = {name: max(1, int(SUBSETS[name] * val_ratio)) for name in SUBSETS}\n",
    "\n",
    "# Sample each to the desired subset size\n",
    "sampled_datasets = {}\n",
    "for name, ds in raw_datasets.items():\n",
    "    subset_size = SUBSETS[name]\n",
    "    total = len(ds)\n",
    "    if subset_size < total:\n",
    "        ds_shuffled = ds.shuffle(seed=42)\n",
    "        sampled = ds_shuffled.select(range(subset_size))\n",
    "    else:\n",
    "        sampled = ds\n",
    "    sampled_datasets[name] = sampled\n",
    "    print(f\"{name}: selected {len(sampled)} of {total}\")\n",
    "\n",
    "sampled_val_datasets = {}\n",
    "for name, ds in raw_val_datasets.items():\n",
    "    subset_size = VAL_SUBSETS[name]\n",
    "    ds_shuffled = ds.shuffle(seed=42)\n",
    "    sampled_val_datasets[name] = ds_shuffled.select(range(subset_size))\n",
    "    print(f\"{name} val: selected {len(sampled_val_datasets[name])} of {len(ds)}\")\n",
    "\n",
    "# 2. Mapping function to unify examples\n",
    "\n",
    "def unify_example(example, source):\n",
    "    record = {\n",
    "        'question': None,\n",
    "        'options': [],\n",
    "        'rationale': '',\n",
    "        'label': None,\n",
    "        'label_idx' : None,\n",
    "        'dataset': source\n",
    "    }\n",
    "    if source == 'openbookqa':\n",
    "        record['question'] = example['question_stem']\n",
    "        texts = example['choices']['text']\n",
    "        for text in texts:\n",
    "            record['options'].append(text)\n",
    "        record['label'] = example['answerKey']\n",
    "        record['label_idx'] = ord(example['answerKey']) - ord('A')\n",
    "        record['rationale'] = \"Key fact:\\n\" + example['fact1']\n",
    "    elif source == 'sciq':\n",
    "        record['question'] = example['question']\n",
    "        options = deque([\n",
    "            example['correct_answer'],\n",
    "            example['distractor1'],\n",
    "            example['distractor2'],\n",
    "            example['distractor3'],\n",
    "        ])\n",
    "        shift = random.randint(0, 3)\n",
    "        options.rotate(shift)\n",
    "        record['options'] = list(options)\n",
    "        record['label'] = chr(ord('A') + shift)\n",
    "        record['label_idx'] = shift\n",
    "        record['rationale'] = \"Supporting evidence:\\n\" + example['support']\n",
    "    elif source == 'race':\n",
    "        record['question'] = example['question']\n",
    "        record['options'] = example['options']\n",
    "        record['label'] = example['answer']\n",
    "        record['label_idx'] = ord(example['answer']) - ord('A')\n",
    "        record['rationale'] = \"Article passage (for context):\\n\"  + example['article']\n",
    "    elif source == 'mmlu_aux':\n",
    "        record['question'] = example['question']\n",
    "        record['options'] = example['choices']\n",
    "        record['label'] = chr(ord('A') + example['answer'])\n",
    "        record['label_idx'] = example['answer']\n",
    "    elif source == 'aqua_rat':\n",
    "        record['question'] = example['question']\n",
    "        record['options'] = [opt[2:] for opt in example['options']]\n",
    "        record['rationale'] = \"Step-by-step solution:\\n\" + example.get('rationale', '')\n",
    "        record['label'] = example['correct']\n",
    "        record['label_idx'] = ord(example['correct']) - ord('A')\n",
    "    elif source == 'medmcqa':\n",
    "        record['question'] = example['question']\n",
    "        record['options']  = [example['opa'], example['opb'], example['opc'], example['opd']]\n",
    "        record['label']    = chr(ord('a') + example['cop'])\n",
    "        record['label_idx'] = example['cop']\n",
    "        record['rationale'] = f\"Explanation:\\n{example['exp']}\" if example['exp'] is not None else ''\n",
    "    return record\n",
    "\n",
    "# 3. Process and unify all sampled datasets\n",
    "unified_datasets = []\n",
    "for name, ds in sampled_datasets.items():\n",
    "    uni = ds.map(lambda ex: unify_example(ex, name), remove_columns=ds.column_names)\n",
    "    unified_datasets.append(uni)\n",
    "combined = concatenate_datasets(unified_datasets)\n",
    "print(f\"Total unified examples: {len(combined)}\")\n",
    "\n",
    "# 4b. Process and unify validation subsets\n",
    "unified_val_datasets = []\n",
    "for name, ds in sampled_val_datasets.items():\n",
    "    uni = ds.map(lambda ex: unify_example(ex, name), remove_columns=ds.column_names)\n",
    "    unified_val_datasets.append(uni)\n",
    "val_combined = concatenate_datasets(unified_val_datasets)\n",
    "print(f\"Total unified val examples: {len(val_combined)}\")\n",
    "\n",
    "# 6. Push du dataset MULTI-COLONNES sur le Hub\n",
    "from datasets import DatasetDict\n",
    "ds = DatasetDict({\n",
    "    \"train\":      combined,\n",
    "    \"validation\": val_combined\n",
    "})\n",
    "\n",
    "ds.push_to_hub(REPO_ID)  # pushes all splits together\n",
    "print(f\"Pushed train + validation splits to {REPO_ID}\")\n",
    "print(f\"Dataset successfully pushed to {REPO_ID}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
