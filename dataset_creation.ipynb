{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0fca694f-9de6-4eaf-8983-06a89ee5c451",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "openbookqa: selected 4900 of 4957\n",
      "sciq: selected 10000 of 11679\n",
      "race: selected 50000 of 87866\n",
      "mmlu_aux: selected 85100 of 99842\n",
      "aqua_rat: selected 50000 of 83671\n",
      "medmcqa: selected 50000 of 182822\n",
      "openbookqa val: selected 13 of 500\n",
      "sciq val: selected 27 of 1000\n",
      "race val: selected 136 of 4887\n",
      "mmlu_aux val: selected 231 of 1531\n",
      "aqua_rat val: selected 136 of 228\n",
      "medmcqa val: selected 136 of 4183\n",
      "Total unified examples: 250000\n",
      "Total unified val examples: 679\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc2fe01c03884ebd81ca4f24f04b5eec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/12 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08e33f8b8add4f7487f2d202e1013433",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/26 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf0918e75f60410b8172024df7272fe3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/131 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbefd7ba7e2543f389cac3df97df62e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/223 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1f72fc4cefe434caa30fe2433d30b8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/131 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fa3f9f6070a4b848c2b962e4ff7ed54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/131 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unified test examples: 654\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c16ffec6f4740e99559d72becb1f8f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cff1273e344540978c516b18c844a8d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/250 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddc5a02ebaed444187bd19c80996edad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ead7309fee4b47a2b13cad3c185186be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4f8316636c540338ab4d1bd8630ecb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4da5facbc9ec46ae9a10efc9922879a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74aaef6f1b0a44f0beb5652c1f145cba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/585 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pushed train + validation + test splits to NicoHelemon/MNLP_M3_mcqa_dataset\n",
      "Dataset successfully pushed to NicoHelemon/MNLP_M3_mcqa_dataset\n"
     ]
    }
   ],
   "source": [
    "# Script to load, sample, unify, and push MCQA datasets to HuggingFace Hub\n",
    "# Replace with your huggingface username\n",
    "\n",
    "HF_USERNAME = \"NicoHelemon\"\n",
    "REPO_ID = f\"{HF_USERNAME}/MNLP_M3_mcqa_dataset\"\n",
    "\n",
    "import os\n",
    "os.environ[\"HF_TOKEN\"] = \"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\"\n",
    "\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "import random\n",
    "from collections import deque\n",
    "\n",
    "# Desired subset sizes per dataset\n",
    "SUBSETS = {\n",
    "    'openbookqa': 4900,\n",
    "    'sciq':      10000,\n",
    "    'race':      50000,\n",
    "    'mmlu_aux':  85100,\n",
    "    'aqua_rat':  50000,\n",
    "    'medmcqa' :  50000\n",
    "}\n",
    "\n",
    "# SUBSETS = {\n",
    "#     'openbookqa': 10,\n",
    "#     'sciq':       10,\n",
    "#     #'race':       10,\n",
    "#     'mmlu_aux':   10,\n",
    "#     'aqua_rat':   10,\n",
    "#     'medmcqa' :   10\n",
    "# }\n",
    "\n",
    "# 1. Load each dataset split (train) and sample subset\n",
    "raw_datasets = {}\n",
    "raw_datasets['openbookqa'] = load_dataset(\"allenai/openbookqa\", name=\"additional\", split='train')\n",
    "raw_datasets['sciq']       = load_dataset(\"allenai/sciq\", split='train')\n",
    "raw_datasets['race']       = load_dataset(\"ehovy/race\", 'all', split='train')\n",
    "raw_datasets['mmlu_aux']   = load_dataset(\"cais/mmlu\", name=\"all\", split='auxiliary_train')\n",
    "raw_datasets['aqua_rat']   = load_dataset(\"NicoHelemon/aqua_rat_4\", split='train')\n",
    "raw_datasets['medmcqa']    = load_dataset(\"openlifescienceai/medmcqa\", split='train')\n",
    "\n",
    "raw_val_datasets = {\n",
    "    'openbookqa': load_dataset(\"allenai/openbookqa\", name=\"additional\", split='validation'),\n",
    "    'sciq':       load_dataset(\"allenai/sciq\", split='validation'),\n",
    "    'race':       load_dataset(\"ehovy/race\", 'all', split='validation'),\n",
    "    'mmlu_aux':   load_dataset(\"cais/mmlu\", name=\"all\", split='validation'),\n",
    "    'aqua_rat':   load_dataset(\"NicoHelemon/aqua_rat_4\", split='validation'),\n",
    "    'medmcqa' :   load_dataset(\"openlifescienceai/medmcqa\", split='validation')\n",
    "}\n",
    "\n",
    "raw_test_datasets = {\n",
    "    'openbookqa': load_dataset(\"allenai/openbookqa\", name=\"additional\", split='test'),\n",
    "    'sciq':       load_dataset(\"allenai/sciq\", split='test'),\n",
    "    'race':       load_dataset(\"ehovy/race\", 'all', split='test'),\n",
    "    'mmlu_aux':   load_dataset(\"cais/mmlu\", name=\"all\", split='test'),\n",
    "    'aqua_rat':   load_dataset(\"NicoHelemon/aqua_rat_4\", split='test'),\n",
    "    'medmcqa':    load_dataset(\"openlifescienceai/medmcqa\", split='test')\n",
    "}\n",
    "\n",
    "actual_ratios = {name: len(raw_val_datasets[name]) / len(raw_datasets[name]) for name in raw_datasets}\n",
    "val_ratio = min(0.05, min(actual_ratios.values()))\n",
    "\n",
    "VAL_SUBSETS = {name: max(1, int(SUBSETS[name] * val_ratio)) for name in SUBSETS}\n",
    "\n",
    "actual_test_ratios = {\n",
    "    name: len(raw_test_datasets[name]) / len(raw_datasets[name])\n",
    "    for name in raw_test_datasets\n",
    "}\n",
    "test_ratio = min(0.05, min(actual_test_ratios.values()))\n",
    "\n",
    "# number of examples per test subset\n",
    "TEST_SUBSETS = {\n",
    "    name: max(1, int(SUBSETS[name] * test_ratio))\n",
    "    for name in SUBSETS\n",
    "}\n",
    "\n",
    "# Sample each to the desired subset size\n",
    "sampled_datasets = {}\n",
    "for name, ds in raw_datasets.items():\n",
    "    subset_size = SUBSETS[name]\n",
    "    total = len(ds)\n",
    "    if subset_size < total:\n",
    "        ds_shuffled = ds.shuffle(seed=42)\n",
    "        sampled = ds_shuffled.select(range(subset_size))\n",
    "    else:\n",
    "        sampled = ds\n",
    "    sampled_datasets[name] = sampled\n",
    "    print(f\"{name}: selected {len(sampled)} of {total}\")\n",
    "\n",
    "sampled_val_datasets = {}\n",
    "for name, ds in raw_val_datasets.items():\n",
    "    subset_size = VAL_SUBSETS[name]\n",
    "    ds_shuffled = ds.shuffle(seed=42)\n",
    "    sampled_val_datasets[name] = ds_shuffled.select(range(subset_size))\n",
    "    print(f\"{name} val: selected {len(sampled_val_datasets[name])} of {len(ds)}\")\n",
    "\n",
    "sampled_test_datasets = {}\n",
    "for name, ds in raw_test_datasets.items():\n",
    "    subset_size = TEST_SUBSETS.get(name, len(ds))\n",
    "    ds_shuffled = ds.shuffle(seed=42)\n",
    "    sampled_test_datasets[name] = ds_shuffled.select(range(subset_size))\n",
    "\n",
    "# 2. Mapping function to unify examples\n",
    "\n",
    "def unify_example(example, source):\n",
    "    record = {\n",
    "        'question': None,\n",
    "        'options': [],\n",
    "        'rationale': '',\n",
    "        'label': None,\n",
    "        'label_idx' : None,\n",
    "        'dataset': source\n",
    "    }\n",
    "    if source == 'openbookqa':\n",
    "        record['question'] = example['question_stem']\n",
    "        texts = example['choices']['text']\n",
    "        for text in texts:\n",
    "            record['options'].append(text)\n",
    "        record['label'] = example['answerKey']\n",
    "        record['label_idx'] = ord(example['answerKey']) - ord('A')\n",
    "        record['rationale'] = \"Key fact:\\n\" + example['fact1']\n",
    "    elif source == 'sciq':\n",
    "        record['question'] = example['question']\n",
    "        options = deque([\n",
    "            example['correct_answer'],\n",
    "            example['distractor1'],\n",
    "            example['distractor2'],\n",
    "            example['distractor3'],\n",
    "        ])\n",
    "        shift = random.randint(0, 3)\n",
    "        options.rotate(shift)\n",
    "        record['options'] = list(options)\n",
    "        record['label'] = chr(ord('A') + shift)\n",
    "        record['label_idx'] = shift\n",
    "        record['rationale'] = \"Supporting evidence:\\n\" + example['support']\n",
    "    elif source == 'race':\n",
    "        record['question'] = example['question']\n",
    "        record['options'] = example['options']\n",
    "        record['label'] = example['answer']\n",
    "        record['label_idx'] = ord(example['answer']) - ord('A')\n",
    "        record['rationale'] = \"Article passage (for context):\\n\"  + example['article']\n",
    "    elif source == 'mmlu_aux':\n",
    "        record['question'] = example['question']\n",
    "        record['options'] = example['choices']\n",
    "        record['label'] = chr(ord('A') + example['answer'])\n",
    "        record['label_idx'] = example['answer']\n",
    "    elif source == 'aqua_rat':\n",
    "        record['question'] = example['question']\n",
    "        record['options'] = [opt[2:] for opt in example['options']]\n",
    "        record['rationale'] = \"Step-by-step solution:\\n\" + example.get('rationale', '')\n",
    "        record['label'] = example['correct']\n",
    "        record['label_idx'] = ord(example['correct']) - ord('A')\n",
    "    elif source == 'medmcqa':\n",
    "        record['question'] = example['question']\n",
    "        record['options']  = [example['opa'], example['opb'], example['opc'], example['opd']]\n",
    "        record['label']    = chr(ord('a') + example['cop'])\n",
    "        record['label_idx'] = example['cop']\n",
    "        record['rationale'] = f\"Explanation:\\n{example['exp']}\" if example['exp'] is not None else ''\n",
    "    return record\n",
    "\n",
    "# 3. Process and unify all sampled datasets\n",
    "unified_datasets = []\n",
    "for name, ds in sampled_datasets.items():\n",
    "    uni = ds.map(lambda ex: unify_example(ex, name), remove_columns=ds.column_names)\n",
    "    unified_datasets.append(uni)\n",
    "combined = concatenate_datasets(unified_datasets)\n",
    "print(f\"Total unified examples: {len(combined)}\")\n",
    "\n",
    "# 4b. Process and unify validation subsets\n",
    "unified_val_datasets = []\n",
    "for name, ds in sampled_val_datasets.items():\n",
    "    uni = ds.map(lambda ex: unify_example(ex, name), remove_columns=ds.column_names)\n",
    "    unified_val_datasets.append(uni)\n",
    "val_combined = concatenate_datasets(unified_val_datasets)\n",
    "print(f\"Total unified val examples: {len(val_combined)}\")\n",
    "\n",
    "unified_test = []\n",
    "for name, ds in sampled_test_datasets.items():\n",
    "    uni = ds.map(lambda ex: unify_example(ex, name), remove_columns=ds.column_names)\n",
    "    unified_test.append(uni)\n",
    "test_combined = concatenate_datasets(unified_test)\n",
    "print(f\"Total unified test examples: {len(test_combined)}\")\n",
    "\n",
    "# 6. Push du dataset MULTI-COLONNES sur le Hub\n",
    "from datasets import DatasetDict\n",
    "ds = DatasetDict({\n",
    "    \"train\":      combined,\n",
    "    \"validation\": val_combined,\n",
    "    \"test\":       test_combined\n",
    "})\n",
    "\n",
    "ds.push_to_hub(REPO_ID)  # pushes all splits together\n",
    "print(f\"Pushed train + validation + test splits to {REPO_ID}\")\n",
    "print(f\"Dataset successfully pushed to {REPO_ID}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15718fe0-1534-4c71-8963-1540a57e51d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17accdbad2ac4f28b7fc3d3091948d24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/97467 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcf5eb52ccc344e8881751a45a08619f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/254 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d53fc50819984757a06c4eeefbe901e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/254 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e0457c03b954c2c93e9c2f0fddd5158",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/83671 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1658cf66407a499fa15576476ce4dc03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/220 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7477a000b49458891906ec32ae58f40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/228 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d965a6b523064b4ca566a33cf1f1ca99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fd6214ef69a4ad188a44d3f3507c540",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/84 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69591459fe524d7297f023506d9e1f81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40f1f6d76fb141b3b630de823c2faea0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4862f31f7d249ef9f9b606c6355927c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70f74cab919c470ab9ad02d9c4a1ee5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset pushed to: NicoHelemon/aqua_rat_4\n"
     ]
    }
   ],
   "source": [
    "# Script to create aqua_rat_4 by filtering out examples where E is correct and removing option E\n",
    "# Replace with your Hugging Face username and token\n",
    "\n",
    "HF_USERNAME = \"NicoHelemon\"\n",
    "REPO_ID = f\"{HF_USERNAME}/aqua_rat_4\"\n",
    "\n",
    "import os\n",
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "# Set your Hugging Face token as an environment variable\n",
    "os.environ[\"HF_TOKEN\"] = \"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\"\n",
    "\n",
    "# 1. Load the full deepmind/aqua_rat dataset (train + validation)\n",
    "datasets = load_dataset(\"deepmind/aqua_rat\")\n",
    "\n",
    "# 2. Filter out any example where the correct answer is 'E'\n",
    "def is_not_e(example):\n",
    "    return example['correct'] != 'E'\n",
    "\n",
    "datasets_filtered = datasets.filter(is_not_e)\n",
    "\n",
    "# 3. Remove the last option (E) from the options list for each remaining example\n",
    "def drop_last_option(example):\n",
    "    # Keep only options A-D\n",
    "    example['options'] = example['options'][:-1]\n",
    "    return example\n",
    "\n",
    "datasets_cleaned = datasets_filtered.map(drop_last_option)\n",
    "\n",
    "# 4. Push the new dataset to the Hub\n",
    "datasets_cleaned.push_to_hub(REPO_ID)\n",
    "\n",
    "print(f\"Dataset pushed to: {REPO_ID}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02698b3b-dba5-41a0-9d6c-b4970e6da16f",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'train'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 52\u001b[0m\n\u001b[1;32m     49\u001b[0m     ds \u001b[38;5;241m=\u001b[39m load_dataset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcais/mmlu\u001b[39m\u001b[38;5;124m\"\u001b[39m, subject)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m splits:\n\u001b[1;32m     51\u001b[0m         \u001b[38;5;66;03m# add the subject column\u001b[39;00m\n\u001b[0;32m---> 52\u001b[0m         ds_tagged \u001b[38;5;241m=\u001b[39m \u001b[43mds\u001b[49m\u001b[43m[\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28;01mlambda\u001b[39;00m ex: {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msubject\u001b[39m\u001b[38;5;124m\"\u001b[39m: subject})\n\u001b[1;32m     53\u001b[0m         merged[split]\u001b[38;5;241m.\u001b[39mappend(ds_tagged)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# concatenate per split\u001b[39;00m\n",
      "File \u001b[0;32m/opt/jlab-env/lib/python3.12/site-packages/datasets/dataset_dict.py:72\u001b[0m, in \u001b[0;36mDatasetDict.__getitem__\u001b[0;34m(self, k)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, k) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dataset:\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(k, (\u001b[38;5;28mstr\u001b[39m, NamedSplit)) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 72\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getitem__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     74\u001b[0m         available_suggested_splits \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     75\u001b[0m             split \u001b[38;5;28;01mfor\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m (Split\u001b[38;5;241m.\u001b[39mTRAIN, Split\u001b[38;5;241m.\u001b[39mTEST, Split\u001b[38;5;241m.\u001b[39mVALIDATION) \u001b[38;5;28;01mif\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n\u001b[1;32m     76\u001b[0m         ]\n",
      "\u001b[0;31mKeyError\u001b[0m: 'train'"
     ]
    }
   ],
   "source": [
    "# requirements:\n",
    "#   pip install datasets huggingface_hub\n",
    "\n",
    "import os\n",
    "from datasets import load_dataset, DatasetDict, concatenate_datasets\n",
    "\n",
    "# your HF credentials and target repo\n",
    "HF_USERNAME = \"NicoHelemon\"\n",
    "REPO_ID      = f\"{HF_USERNAME}/mmlu_STEM\"\n",
    "os.environ[\"HF_TOKEN\"] = \"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\"\n",
    "\n",
    "# list of the STEM subsets you want to merge\n",
    "stem_subsets = [\n",
    "    \"abstract_algebra\",\n",
    "    \"elementary_mathematics\",\n",
    "    \"high_school_mathematics\",\n",
    "    \"college_mathematics\",\n",
    "    \"high_school_statistics\",\n",
    "    \"formal_logic\",\n",
    "    \"anatomy\",\n",
    "    \"astronomy\",\n",
    "    \"college_biology\",\n",
    "    \"high_school_biology\",\n",
    "    \"college_chemistry\",\n",
    "    \"high_school_chemistry\",\n",
    "    \"conceptual_physics\",\n",
    "    \"college_physics\",\n",
    "    \"high_school_physics\",\n",
    "    \"medical_genetics\",\n",
    "    \"nutrition\",\n",
    "    \"human_aging\",\n",
    "    \"virology\",\n",
    "    \"clinical_knowledge\",\n",
    "    \"electrical_engineering\",\n",
    "    \"college_computer_science\",\n",
    "    \"high_school_computer_science\",\n",
    "    \"computer_security\",\n",
    "    \"machine_learning\",\n",
    "]\n",
    "\n",
    "# the splits in the original mmlu\n",
    "splits = [\"test\", \"validation\", \"dev\"]\n",
    "\n",
    "# placeholder lists for each split\n",
    "merged = { split: [] for split in splits }\n",
    "\n",
    "# load each subject, tag it, and collect\n",
    "for subject in stem_subsets:\n",
    "    ds = load_dataset(\"cais/mmlu\", subject)\n",
    "    for split in splits:\n",
    "        # add the subject column\n",
    "        ds_tagged = ds[split].map(lambda ex: {\"subject\": subject})\n",
    "        merged[split].append(ds_tagged)\n",
    "\n",
    "# concatenate per split\n",
    "final_ds = DatasetDict({\n",
    "    split: concatenate_datasets(merged[split]) \n",
    "    for split in splits\n",
    "})\n",
    "\n",
    "# (optional) shuffle each split if you like:\n",
    "# final_ds = final_ds.map(lambda x: x, shuffle=True)\n",
    "\n",
    "# push to your HF Hub repo (will pick up HF_TOKEN from env)\n",
    "final_ds.push_to_hub(REPO_ID, \n",
    "                     token=os.environ[\"HF_TOKEN\"], \n",
    "                     private=False)  # or True if you want it private\n",
    "\n",
    "print(f\"✅ Dataset successfully pushed to https://huggingface.co/{REPO_ID}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
