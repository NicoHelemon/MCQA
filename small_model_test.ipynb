{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e9274de6-46ad-4339-b9f5-a88b4b7c7c04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Qwen3ForSequenceClassification were not initialized from the model checkpoint at Qwen/Qwen3-0.6B-Base and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 594.00 MiB. GPU 0 has a total capacity of 19.50 GiB of which 6.81 MiB is free. Process 1080787 has 1.89 GiB memory in use. Process 1207723 has 1.89 GiB memory in use. Process 1797640 has 1.12 GiB memory in use. Process 1853060 has 1.48 GiB memory in use. Process 1649597 has 348.00 MiB memory in use. Process 1916197 has 10.61 GiB memory in use. Process 1978994 has 1.98 GiB memory in use. Process 2096171 has 744.00 MiB memory in use. Process 2128560 has 1.89 GiB memory in use. Process 2181713 has 1.89 GiB memory in use. Process 3032836 has 17.48 GiB memory in use. Process 3072102 has 4.32 GiB memory in use. Process 3094191 has 13.77 GiB memory in use. Of the allocated memory 17.12 GiB is allocated by PyTorch, and 259.14 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 95\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;66;03m# 5. Trainer setup\u001b[39;00m\n\u001b[1;32m     85\u001b[0m training_args \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[1;32m     86\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39mOUTPUT_DIR,                       \u001b[38;5;66;03m#  Where to save checkpoints and logs\u001b[39;00m\n\u001b[1;32m     87\u001b[0m     num_train_epochs\u001b[38;5;241m=\u001b[39mNUM_EPOCHS,                 \u001b[38;5;66;03m#  Total number of training epochs\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     92\u001b[0m     push_to_hub\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,                           \u001b[38;5;66;03m#  Disable pushing to Hugging Face Hub\u001b[39;00m\n\u001b[1;32m     93\u001b[0m )\n\u001b[0;32m---> 95\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[43mTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m                                 \u001b[49m\u001b[38;5;66;43;03m#  Model to train\u001b[39;49;00m\n\u001b[1;32m     97\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m                         \u001b[49m\u001b[38;5;66;43;03m#  Training configuration\u001b[39;49;00m\n\u001b[1;32m     98\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenized\u001b[49m\u001b[43m,\u001b[49m\u001b[43m                    \u001b[49m\u001b[38;5;66;43;03m#  Prepared training dataset\u001b[39;49;00m\n\u001b[1;32m     99\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_collator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_data_collator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m        \u001b[49m\u001b[38;5;66;43;03m#  Function to batch examples    \u001b[39;49;00m\n\u001b[1;32m    100\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;66;03m# 6. Train & save\u001b[39;00m\n\u001b[1;32m    103\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()                \u001b[38;5;66;03m#  Run the training loop\u001b[39;00m\n",
      "File \u001b[0;32m~/my_venvs/mnlp_m2/lib/python3.12/site-packages/transformers/utils/deprecation.py:172\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action\u001b[38;5;241m.\u001b[39mNOTIFY, Action\u001b[38;5;241m.\u001b[39mNOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[1;32m    170\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/my_venvs/mnlp_m2/lib/python3.12/site-packages/transformers/trainer.py:614\u001b[0m, in \u001b[0;36mTrainer.__init__\u001b[0;34m(self, model, args, data_collator, train_dataset, eval_dataset, processing_class, model_init, compute_loss_func, compute_metrics, callbacks, optimizers, optimizer_cls_and_kwargs, preprocess_logits_for_metrics)\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[38;5;66;03m# Bnb Quantized models doesn't support `.to` operation.\u001b[39;00m\n\u001b[1;32m    610\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    611\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mplace_model_on_device\n\u001b[1;32m    612\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquantization_method\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m QuantizationMethod\u001b[38;5;241m.\u001b[39mBITS_AND_BYTES\n\u001b[1;32m    613\u001b[0m ):\n\u001b[0;32m--> 614\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_move_model_to_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;66;03m# Force n_gpu to 1 to avoid DataParallel as MP will manage the GPUs\u001b[39;00m\n\u001b[1;32m    617\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_model_parallel:\n",
      "File \u001b[0;32m~/my_venvs/mnlp_m2/lib/python3.12/site-packages/transformers/trainer.py:901\u001b[0m, in \u001b[0;36mTrainer._move_model_to_device\u001b[0;34m(self, model, device)\u001b[0m\n\u001b[1;32m    900\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_move_model_to_device\u001b[39m(\u001b[38;5;28mself\u001b[39m, model, device):\n\u001b[0;32m--> 901\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    902\u001b[0m     \u001b[38;5;66;03m# Moving a model to an XLA device disconnects the tied weights, so we have to retie them.\u001b[39;00m\n\u001b[1;32m    903\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mparallel_mode \u001b[38;5;241m==\u001b[39m ParallelMode\u001b[38;5;241m.\u001b[39mTPU \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtie_weights\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/my_venvs/mnlp_m2/lib/python3.12/site-packages/transformers/modeling_utils.py:3698\u001b[0m, in \u001b[0;36mPreTrainedModel.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3693\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_present_in_args:\n\u001b[1;32m   3694\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   3695\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou cannot cast a GPTQ model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3696\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `dtype` by passing the correct `torch_dtype` argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3697\u001b[0m         )\n\u001b[0;32m-> 3698\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/jlab-env-3.12.8/lib/python3.12/site-packages/torch/nn/modules/module.py:1340\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1337\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1338\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m-> 1340\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/jlab-env-3.12.8/lib/python3.12/site-packages/torch/nn/modules/module.py:900\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    899\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 900\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    902\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    903\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    904\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    905\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    910\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    911\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/jlab-env-3.12.8/lib/python3.12/site-packages/torch/nn/modules/module.py:900\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    899\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 900\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    902\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    903\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    904\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    905\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    910\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    911\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/jlab-env-3.12.8/lib/python3.12/site-packages/torch/nn/modules/module.py:927\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    923\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    924\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    925\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    926\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 927\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    928\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    930\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[0;32m/opt/jlab-env-3.12.8/lib/python3.12/site-packages/torch/nn/modules/module.py:1326\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1320\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m   1321\u001b[0m             device,\n\u001b[1;32m   1322\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1323\u001b[0m             non_blocking,\n\u001b[1;32m   1324\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[1;32m   1325\u001b[0m         )\n\u001b[0;32m-> 1326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1327\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1328\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1329\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1330\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1331\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1332\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 594.00 MiB. GPU 0 has a total capacity of 19.50 GiB of which 6.81 MiB is free. Process 1080787 has 1.89 GiB memory in use. Process 1207723 has 1.89 GiB memory in use. Process 1797640 has 1.12 GiB memory in use. Process 1853060 has 1.48 GiB memory in use. Process 1649597 has 348.00 MiB memory in use. Process 1916197 has 10.61 GiB memory in use. Process 1978994 has 1.98 GiB memory in use. Process 2096171 has 744.00 MiB memory in use. Process 2128560 has 1.89 GiB memory in use. Process 2181713 has 1.89 GiB memory in use. Process 3032836 has 17.48 GiB memory in use. Process 3072102 has 4.32 GiB memory in use. Process 3094191 has 13.77 GiB memory in use. Of the allocated memory 17.12 GiB is allocated by PyTorch, and 259.14 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Tiny‐scale MCQA fine‐tuning on Qwen3: pick A–E over 50 examples.\n",
    "\"\"\"\n",
    "import os                            # 0. Interact with the operating system\n",
    "from datasets import load_dataset   #  Load dataset utilities from Hugging Face\n",
    "from transformers import (\n",
    "    AutoTokenizer,                  #  Tokenizer for converting text to model inputs\n",
    "    AutoModelForSequenceClassification,  #  Model class for sequence classification tasks\n",
    "    Trainer,                        #  Trainer API for training/evaluation\n",
    "    TrainingArguments,              #  Argument class for training configuration\n",
    "    default_data_collator,          #  Data collator for batching inputs\n",
    ")\n",
    "\n",
    "# 0. Auth\n",
    "HF_TOKEN = \"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\"\n",
    "os.environ[\"HF_TOKEN\"] = HF_TOKEN  #  Set HF API token in environment\n",
    "\n",
    "# 1. Configuration\n",
    "MODEL_NAME    = \"Qwen/Qwen3-0.6B-Base\"      #  Pretrained model identifier\n",
    "DATASET_NAME  = \"NicoHelemon/MNLP_M2_mcqa_dataset\"  #  MCQA dataset identifier\n",
    "OUTPUT_DIR    = \"tmp_small_mcqa\"           #  Directory for saving outputs\n",
    "MAX_LENGTH    = 256                        #  Max token length per input\n",
    "BATCH_SIZE    = 4                          #  Training batch size per device\n",
    "NUM_EPOCHS    = 50                         #  Number of training epochs\n",
    "LEARNING_RATE = 5e-5                       #  Learning rate for optimizer\n",
    "SMALL_SIZE    = 50                         #  Subsample size from dataset\n",
    "NUM_LABELS    = 5    # A–E                 #  Number of answer choices (A–E)\n",
    "\n",
    "# 2. Load & subsample\n",
    "raw = load_dataset(DATASET_NAME, split=\"train\")  #  Load training split of dataset\n",
    "small_raw = raw.shuffle(seed=42).select(range(SMALL_SIZE))  #  Shuffle and select first SMALL_SIZE examples\n",
    "\n",
    "# 3. Tokenizer & model for classification\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_NAME, \n",
    "    trust_remote_code=True,  #  Allow running custom code from the model repo\n",
    "    token=HF_TOKEN           #  Use HF token for authentication\n",
    ")\n",
    "\n",
    "# <— ADD THESE TWO LINES:\n",
    "tokenizer.pad_token     = tokenizer.eos_token           #  Set padding token to end-of-sequence token\n",
    "tokenizer.pad_token_id  = tokenizer.eos_token_id        #  Set padding token ID accordingly\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    trust_remote_code=True,  #  Allow custom model code\n",
    "    token=HF_TOKEN,          #  Authenticate\n",
    "    num_labels=NUM_LABELS,   #  Define number of classification labels\n",
    ")\n",
    "\n",
    "# Ensure the model knows the pad token ID\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "# 4. Preprocessing: one prompt string → one label 0–4\n",
    "def preprocess(ex):\n",
    "    q    = ex[\"question\"]                       #  Extract question text\n",
    "    opts = ex[\"options\"]                        #  Extract list of 5 option strings\n",
    "    # Build a single prompt with all options\n",
    "    prompt_lines = [f\"Question: {q}\", \"Options:\"]  #  Start prompt with question and header\n",
    "    labels = [\"A\",\"B\",\"C\",\"D\",\"E\"]         #  List of answer labels\n",
    "    for L, o in zip(labels, opts):\n",
    "        prompt_lines.append(f\"{L}. {o}\")           #  Append each label-option pair\n",
    "    prompt_lines.append(\"Answer (letter only):\")    #  Prompt for answer letter\n",
    "    prompt = \"\\n\".join(prompt_lines)              #  Join lines into a single string\n",
    "\n",
    "    enc = tokenizer(\n",
    "        prompt,\n",
    "        truncation=True,          #  Truncate inputs longer than MAX_LENGTH\n",
    "        padding=\"max_length\",   #  Pad inputs to MAX_LENGTH\n",
    "        max_length=MAX_LENGTH,    #  Maximum sequence length\n",
    "    )\n",
    "    return {\n",
    "        \"input_ids\":      enc[\"input_ids\"],          #  Token IDs\n",
    "        \"attention_mask\": enc[\"attention_mask\"],     #  Attention mask for padding\n",
    "        # map \"A\"→0, …, \"E\"→4\n",
    "        \"labels\":         ord(ex[\"label\"]) - ord(\"A\"),  #  Convert label letter to index\n",
    "    }\n",
    "\n",
    "# Apply preprocessing to dataset, removing original columns\n",
    "tokenized = small_raw.map(preprocess, remove_columns=small_raw.column_names)\n",
    "\n",
    "# 5. Trainer setup\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,                       #  Where to save checkpoints and logs\n",
    "    num_train_epochs=NUM_EPOCHS,                 #  Total number of training epochs\n",
    "    per_device_train_batch_size=BATCH_SIZE,      #  Batch size per GPU/CPU\n",
    "    learning_rate=LEARNING_RATE,                 #  Learning rate\n",
    "    logging_steps=5,                            #  Log every 10 steps\n",
    "    save_steps=50,                               #  Save checkpoint every 50 steps\n",
    "    push_to_hub=False,                           #  Disable pushing to Hugging Face Hub\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,                                 #  Model to train\n",
    "    args=training_args,                         #  Training configuration\n",
    "    train_dataset=tokenized,                    #  Prepared training dataset\n",
    "    data_collator=default_data_collator,        #  Function to batch examples    \n",
    ")\n",
    "\n",
    "# 6. Train & save\n",
    "trainer.train()                #  Run the training loop\n",
    "model.save_pretrained(OUTPUT_DIR)  #  Save final model weights\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)  #  Save tokenizer configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72ad6cc2-1425-4894-a6ab-eb58c7162ef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU cores (logical): 128\n",
      "CPU cores (physical): 64\n",
      "GPUs available: 1\n",
      " GPU 0: NVIDIA A100-SXM4-80GB MIG 1g.20gb\n",
      "Accelerate reports: OrderedDict({'': 0})\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import multiprocessing\n",
    "\n",
    "# CPU\n",
    "print(\"CPU cores (logical):\", os.cpu_count())\n",
    "print(\"CPU cores (physical):\", multiprocessing.cpu_count() // 2)  # roughly, on hyperthreaded machines\n",
    "\n",
    "# PyTorch GPU check\n",
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    n_gpu = torch.cuda.device_count()\n",
    "    print(\"GPUs available:\", n_gpu)\n",
    "    for i in range(n_gpu):\n",
    "        print(f\" GPU {i}:\", torch.cuda.get_device_name(i))\n",
    "else:\n",
    "    print(\"No CUDA GPUs detected\")\n",
    "\n",
    "# Hugging Face / Accelerate\n",
    "from accelerate import notebook_launcher, infer_auto_device_map\n",
    "print(\"Accelerate reports:\", infer_auto_device_map(torch.nn.Linear(1,1)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (mnlp_m2)",
   "language": "python",
   "name": "mnlp_m2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
