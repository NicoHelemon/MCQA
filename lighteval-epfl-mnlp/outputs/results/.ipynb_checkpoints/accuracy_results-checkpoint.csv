model_name,metric,community|mmlu:stem,community|mnlp_mcqa_evals,helm|commonsenseqa,helm|med_qa,original|arc:c:letters,all_community,lighteval|agieval:aqua-rat,lighteval|openbookqa,lighteval|race:high,lighteval|sciq,all_lighteval
Qwen/Qwen3-0.6B,acc,0.3673657524775294,0.28,0.20638820638820637,0.20707269155206287,0.23976109215017063,0.26011754851359387,,,,,
Qwen/Qwen3-0.6B-Base,acc,0.48766997003917956,0.42,0.3194103194103194,0.22593320235756384,0.3003412969283277,0.35067095774707807,,,,,
NicoHelemon/MNLP_M2_mcqa_model,acc,0.5243143581470385,0.42,0.2588042588042588,0.20982318271119843,0.40187713310580203,0.3629637865536596,,,,,
NicoHelemon/MNLP_M2_mcqa_model_cot08,acc,0.48766997003917956,0.42,0.3194103194103194,0.22593320235756384,0.3003412969283277,0.35067095774707807,,,,,
NicoHelemon/MNLP_M2_mcqa_model_cot00,acc,0.4694630099101175,0.36,0.5675675675675675,0.2636542239685658,0.5170648464163823,0.43554992957252664,,,,,
NicoHelemon/MNLP_M2_mcqa_model_cot02,acc,0.48766997003917956,0.42,0.3194103194103194,0.22593320235756384,0.3003412969283277,0.35067095774707807,,,,,
NicoHelemon/MNLP_M2_mcqa_model_cot05,acc,0.5040331873703618,0.32,0.3677313677313677,0.21886051080550098,0.5008532423208191,0.3822956616456099,,,,,
